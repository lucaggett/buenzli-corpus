{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2023-07-05T10:50:26.554878Z",
     "start_time": "2023-07-05T10:50:26.552526Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import xml.etree.ElementTree as ET\n",
    "from copy import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wiki.xml\n",
      "schobinger.xml\n",
      "blick.xml\n",
      "swatch.xml\n",
      "blogs.xml\n"
     ]
    }
   ],
   "source": [
    "# get all files in path\n",
    "files = (file for file in os.listdir(\"../NOAH-Corpus\") if file.endswith(\".xml\"))\n",
    "# create dictionary for words and their frequencies\n",
    "words = {}\n",
    "\n",
    "# iterate through files\n",
    "for file in files:\n",
    "    # parse xml file\n",
    "    print(file)\n",
    "    tree = ET.parse(open(\"../NOAH-Corpus/\" + file, \"r\", encoding=\"utf-8\"))\n",
    "    # get root element\n",
    "    root = tree.getroot()\n",
    "    # iterate through all text elements\n",
    "    for article in root.iter(\"document\"):\n",
    "        for sent in article.iter(\"s\"):\n",
    "            for word_element in sent.iter(\"w\"):\n",
    "                # get word\n",
    "                word = word_element.text\n",
    "                word = word.lower()\n",
    "                # if word is not in dictionary, add it\n",
    "                if word not in words:\n",
    "                    words[word] = 1\n",
    "                # else increment frequency\n",
    "                else:\n",
    "                    words[word] += 1\n",
    "\n",
    "\n",
    "sorted(words.items(), key=lambda x: x[1], reverse=True)\n",
    "NOAH_frequencies = copy(words)\n",
    "del words"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-05T00:58:05.827422Z",
     "start_time": "2023-07-05T00:58:05.525304Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5.176436154773465\n",
      "[('und', 21197), ('isch', 18917), ('de', 17210), ('das', 15846), ('es', 11269), ('die', 10940), ('aber', 10045), ('ich', 9433), ('i', 9038), ('au', 8884), ('so', 8701), ('wo', 8695), ('d', 7945), ('vo', 7800), ('', 7623), ('mit', 7364), ('en', 6580), ('du', 6410), ('dass', 5948), ('für', 5931), ('e', 5922), ('nid', 5753), ('im', 5519), ('uf', 5517), ('nöd', 5400), ('mer', 5350), ('no', 5232), ('was', 5061), ('wie', 5059), ('ja', 5011), ('in', 4870), ('oder', 4848), ('ned', 4623), ('scho', 4609), ('als', 4416), ('sind', 4243), ('wenn', 4241), ('bi', 3887), ('z', 3862), ('s', 3830), ('eifach', 3790), ('da', 3780), ('het', 3743), ('sich', 3694), ('nur', 3492), ('denn', 3428), ('mir', 3310), ('zum', 3225), ('am', 3186), ('si', 3165), ('der', 3107), ('sie', 3043), ('dr', 2892), ('ha', 2859), ('a', 2783), ('di', 2619), ('kei', 2505), ('mal', 2500), ('me', 2448), ('immer', 2393), ('zu', 2319), ('meh', 2297), ('dem', 2253), ('doch', 2206), ('dä', 2154), ('vom', 2135), ('er', 2117), ('wird', 2020), ('han', 2017), ('us', 1968), ('jetzt', 1921), ('grad', 1858), ('halt', 1836), ('lüt', 1811), ('ganz', 1801), ('esch', 1777), ('-', 1766), ('hesch', 1762), ('vor', 1754), ('mi', 1709), ('hend', 1682), ('will', 1671), ('jo', 1649), ('über', 1581), ('also', 1534), ('ond', 1496), ('guet', 1495), ('the', 1482), ('cha', 1475), ('u', 1472), ('ds', 1434), ('um', 1426), ('chli', 1413), ('alli', 1405), ('bin', 1390), ('hani', 1384), ('em', 1383), ('alles', 1373), ('wär', 1347), ('do', 1337)]\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from copy import copy\n",
    "with open(\"../buenzli-corpus/comments.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    comments = json.load(f)\n",
    "\n",
    "word_frequencies = {}\n",
    "for comment in comments:\n",
    "    sentence = comment[\"body\"]\n",
    "    for word in sentence.split(\" \"):\n",
    "        word = word.lower()\n",
    "        if word not in word_frequencies:\n",
    "            word_frequencies[word] = 1\n",
    "        else:\n",
    "            word_frequencies[word] += 1\n",
    "\n",
    "total_words = sum(word_frequencies.values())\n",
    "\n",
    "average_word_length = sum(len(word) * freq for word, freq in word_frequencies.items()) / total_words\n",
    "\n",
    "print(average_word_length, sep=\"\\n\")\n",
    "\n",
    "\n",
    "print(sorted(word_frequencies.items(), key=lambda x: x[1], reverse=True)[:100])\n",
    "\n",
    "buenzli_frequencies = copy(word_frequencies)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-05T00:58:09.646449Z",
     "start_time": "2023-07-05T00:58:09.282584Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "380846 951064 370105 22526\n",
      "other langs 1654494\n",
      "other langs - GSW 1650636\n",
      "2482 2878 2500 9680 144401\n"
     ]
    }
   ],
   "source": [
    "\n",
    "words_fr = set()\n",
    "with open(\"../dicts/dictionary.csv\") as f:\n",
    "    for line in f:\n",
    "        for word in line.split():\n",
    "            word = word.lower()\n",
    "            words_fr.add(word)\n",
    "\n",
    "words_it = set()\n",
    "with open(\"../dicts/parole_uniche.txt\") as f:\n",
    "    for line in f:\n",
    "        for word in line.split():\n",
    "            word = word.lower()\n",
    "            words_it.add(word)\n",
    "\n",
    "words_en = set()\n",
    "with open(\"../dicts/words_alpha.txt\") as f:\n",
    "    for line in f:\n",
    "        for word in line.split():\n",
    "            word = word.lower()\n",
    "            words_en.add(word)\n",
    "\n",
    "with open(\"../scripts/comments.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    comments = json.load(f)\n",
    "\n",
    "words_GSW = set(NOAH_frequencies.keys())\n",
    "\n",
    "\n",
    "print(len(words_fr), len(words_it), len(words_en), len(words_GSW))\n",
    "print(\"other langs\", len((words_fr.union(words_it).union(words_en))))\n",
    "print(\"other langs - GSW\", len((words_fr.union(words_it).union(words_en).difference(words_GSW))))\n",
    "\n",
    "IT_in_buenzli = {}\n",
    "FR_in_buenzli = {}\n",
    "EN_in_buenzli = {}\n",
    "GSW_in_buenzli = {}\n",
    "other_in_buenzli = {}\n",
    "\n",
    "for comment in comments:\n",
    "    if comment[\"language\"] != \"GSW\":\n",
    "        #print(f\"Skipping {comment['language']} comment\")\n",
    "        continue\n",
    "    sentence = comment[\"body\"]\n",
    "    for word in sentence.split(\" \"):\n",
    "        word = word.lower()\n",
    "        if word in words_GSW:\n",
    "            if word not in GSW_in_buenzli:\n",
    "                GSW_in_buenzli[word] = [comment[\"body\"]]\n",
    "            else:\n",
    "                GSW_in_buenzli[word].append(comment[\"body\"])\n",
    "        elif word in words_fr:\n",
    "            if word not in FR_in_buenzli:\n",
    "                FR_in_buenzli[word] = [comment[\"body\"]]\n",
    "            else:\n",
    "                FR_in_buenzli[word].append(comment[\"body\"])\n",
    "        elif word in words_it:\n",
    "            if word not in IT_in_buenzli:\n",
    "                IT_in_buenzli[word] = [comment[\"body\"]]\n",
    "            else:\n",
    "                IT_in_buenzli[word].append(comment[\"body\"])\n",
    "        elif word in words_en:\n",
    "            if word not in EN_in_buenzli:\n",
    "                EN_in_buenzli[word] = [comment[\"body\"]]\n",
    "            else:\n",
    "                EN_in_buenzli[word].append(comment[\"body\"])\n",
    "        else:\n",
    "            if word not in other_in_buenzli:\n",
    "                other_in_buenzli[word] = [comment[\"body\"]]\n",
    "            else:\n",
    "                other_in_buenzli[word].append(comment[\"body\"])\n",
    "\n",
    "\n",
    "print(len(FR_in_buenzli), len(IT_in_buenzli), len(EN_in_buenzli), len(GSW_in_buenzli), len(other_in_buenzli))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-05T11:19:50.004548Z",
     "start_time": "2023-07-05T11:19:48.318042Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "3\n",
      "114\n",
      "14\n",
      "308\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Utility data structures\n",
    "\"\"\"\n",
    "\n",
    "buenzli_words = set(buenzli_frequencies.keys())\n",
    "NOAH_words = set(NOAH_frequencies.keys())\n",
    "\n",
    "buenzli_words_not_in_NOAH = buenzli_words - NOAH_words\n",
    "NOAH_words_not_in_buenzli = NOAH_words - buenzli_words\n",
    "\n",
    "normalized_NOAH_frequencies = {word: freq / len(NOAH_words) for word, freq in NOAH_frequencies.items()}\n",
    "normalized_buenzli_frequencies = {word: freq / len(buenzli_words) for word, freq in buenzli_frequencies.items()}\n",
    "\n",
    "print(buenzli_frequencies[\"kreiswichs\"])\n",
    "print(buenzli_frequencies[\"chreiswichs\"]) # A literal german translation of the english \"circlejerk\"\n",
    "print(buenzli_frequencies[\"pfoste\"])\n",
    "print(buenzli_frequencies[\"pfostiere\"])\n",
    "print(buenzli_frequencies[\"post\"])\n",
    "print(buenzli_frequencies[\"enjoyer\"])\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-05T10:57:08.741209Z",
     "start_time": "2023-07-05T10:57:08.712561Z"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
