{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2023-07-05T00:42:22.433952Z",
     "start_time": "2023-07-05T00:42:22.432059Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import xml.etree.ElementTree as ET\n",
    "from copy import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wiki.xml\n",
      "schobinger.xml\n",
      "blick.xml\n",
      "swatch.xml\n",
      "blogs.xml\n"
     ]
    }
   ],
   "source": [
    "# get all files in path\n",
    "files = (file for file in os.listdir(\"../NOAH-Corpus\") if file.endswith(\".xml\"))\n",
    "# create dictionary for words and their frequencies\n",
    "words = {}\n",
    "\n",
    "# iterate through files\n",
    "for file in files:\n",
    "    # parse xml file\n",
    "    print(file)\n",
    "    tree = ET.parse(open(\"../NOAH-Corpus/\" + file, \"r\", encoding=\"utf-8\"))\n",
    "    # get root element\n",
    "    root = tree.getroot()\n",
    "    # iterate through all text elements\n",
    "    for article in root.iter(\"document\"):\n",
    "        for sent in article.iter(\"s\"):\n",
    "            for word_element in sent.iter(\"w\"):\n",
    "                # get word\n",
    "                word = word_element.text\n",
    "                # if word is not in dictionary, add it\n",
    "                if word not in words:\n",
    "                    words[word_element] = 1\n",
    "                # else increment frequency\n",
    "                else:\n",
    "                    words[word_element] += 1\n",
    "\n",
    "\n",
    "sorted(words.items(), key=lambda x: x[1], reverse=True)\n",
    "NOAH_frequencies = copy(words)\n",
    "del words"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-05T00:46:51.852883Z",
     "start_time": "2023-07-05T00:46:51.450744Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5.176436154773465\n",
      "[('und', 21197), ('isch', 18917), ('de', 17210), ('das', 15846), ('es', 11269), ('die', 10940), ('aber', 10045), ('ich', 9433), ('i', 9038), ('au', 8884), ('so', 8701), ('wo', 8695), ('d', 7945), ('vo', 7800), ('', 7623), ('mit', 7364), ('en', 6580), ('du', 6410), ('dass', 5948), ('f√ºr', 5931), ('e', 5922), ('nid', 5753), ('im', 5519), ('uf', 5517), ('n√∂d', 5400), ('mer', 5350), ('no', 5232), ('was', 5061), ('wie', 5059), ('ja', 5011), ('in', 4870), ('oder', 4848), ('ned', 4623), ('scho', 4609), ('als', 4416), ('sind', 4243), ('wenn', 4241), ('bi', 3887), ('z', 3862), ('s', 3830), ('eifach', 3790), ('da', 3780), ('het', 3743), ('sich', 3694), ('nur', 3492), ('denn', 3428), ('mir', 3310), ('zum', 3225), ('am', 3186), ('si', 3165), ('der', 3107), ('sie', 3043), ('dr', 2892), ('ha', 2859), ('a', 2783), ('di', 2619), ('kei', 2505), ('mal', 2500), ('me', 2448), ('immer', 2393), ('zu', 2319), ('meh', 2297), ('dem', 2253), ('doch', 2206), ('d√§', 2154), ('vom', 2135), ('er', 2117), ('wird', 2020), ('han', 2017), ('us', 1968), ('jetzt', 1921), ('grad', 1858), ('halt', 1836), ('l√ºt', 1811), ('ganz', 1801), ('esch', 1777), ('-', 1766), ('hesch', 1762), ('vor', 1754), ('mi', 1709), ('hend', 1682), ('will', 1671), ('jo', 1649), ('√ºber', 1581), ('also', 1534), ('ond', 1496), ('guet', 1495), ('the', 1482), ('cha', 1475), ('u', 1472), ('ds', 1434), ('um', 1426), ('chli', 1413), ('alli', 1405), ('bin', 1390), ('hani', 1384), ('em', 1383), ('alles', 1373), ('w√§r', 1347), ('do', 1337)]\n",
      "\n",
      "swatch.com\n",
      "180435\n",
      "17603\n",
      "['wiiblich?', 'herte', 'platzü§∑üèº\\u200d‚ôÇÔ∏è', 'charr√§...', 'ungerbew√§rtete', 'ablah.', 'b√ºndnercorona,', 'urteil', 'vauwe', 'stessa', 'mache)', 'drzuegh√∂re.', 'ch√∂ngsh', 'redditforum.\\n\\ngiz', 'ahhaha', 'schlossh√∂fli', 'abfall?\\n\\nbi', 'glutscht', 'verruckt.', 'verw√ºtsche', '[ke', 'duresetzt.\\n\\nwird', 'mod√§ll', 'schluss...', 'pipifax', 'ausreichende', 'villes', 'planet?', 'tah.', '(aldichinder).', 'cheggt?', 'vorarlberg?', 'tammi,', '19.921875%', \"d'laminierig\", 'digitec!', 'ez,', 'z√§mearbet\\n\\n√§hm,', 'schiissirinne', 'mangelware', 'aadrang', 'damer', 'ohhhhh', 'selbsternannte', 'jedesmou', 'stachel', 'ikommensklasse).', 'guetm√ºetige', 'z‚Äôreise!\\n\\n185,', 'blickinterview', '\\nliechteschtei', 'mietwage', 'prompt', '\"punktifiziere\"', 'meischde', 'umbennent', 'titel!\\n', 'liebe....', 'bachets', 'uufghalte', 'gschmeidig!', 'huusboot,', 'jed', 'f√ºren√§h.\\naber', 'hanget.', 'zuegworfe', '0:11', 'gricht', 'straffmass', 'randeguezli', 'money.', 'postid...', '1.\\tjesus', 'schultere', 'glieferet!', 'impfst√∂ff(zb', 'subreddits', 'chmedia', 'duregspielt', '*es', 'melke', 'asatz', '/u/eipa,', 'vereinsamte', 'unpers√∂nlicher', 'zha‚Ä¶', 'go,', 'traumcharte.', 'nachbarsl√§nder,', 'www.tagesanzeiger.ch', 'huh?', 'mutiere.\\n\\nd', 'beklagar,', 'nirgens', 'begr√ºnde,', 'w√§utherrschaft?', 'arme...', 'widerstah.', '(dateschutz', 'sou.']\n",
      "**************************************************************************************************** \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "['Christopher', 'H√πftoperation', 'Official', 'cina', 'Dracheriter', 'userucke', 'Omega-logo', 'W√®r', 'gas√¥kun', 'ortsunabh√§ngig', 'zahlriichi', 'Kongr√§ss', 'Schauschpielerin', 'Neverunderdressed', 'harzig', 'Schpineri', 'Sunntig', 'Pariser', 'Monica', 'Leischtigsstufe', 'ufspr√ºtze', 'Shop-in-shop-Netz', 'f√ºrspeiende', 'notturna', 'Kampffeld', 'Gangreserve', '√§inzig', 'Ishockeyf√§uder', 'g√§gen', 'Tour', 'ix', 'grossz√∂gig', 'Verteidigigsministerium', 'Motiv', 'Rutschbahn', 'Technikgenie', 'Ribolla', 'ihr√§s', 'Short', 'Hufe', 'gh√ºut', 'aavertroue', 'Letizia', 'Mango', 'Mark√§', 'Thema', 'Dr√º√ºzaiger-Uhr√§', 'Erch√§nigsdienscht', 'F√∂rderig', 'Erste', 'Liiche', 'Verbr√§che', 'unredliche', 'Omega-uhrwerch', 'Absatzmarkt', 'Baden-Baden', 'Verb', 'Gangres√§rv√§az√§ig', 'Aarauer', 'T√π√πrnschtund', 'S√§ltne', 'Chauf', 'inestoh', 'Juli', 'volatil√§', 'babylonische', 'Informationveraastaltige', 'S√∂ldner', 'Smalltalk', 'Koffere', 'vuner√§', 'Schport', 'F√§hlerhafte', 'mehrer√§', 'schlechtischt', 'Natione', 'Ic', 'Kl√§ng', 'Nyc', 'K√∂nstler', 'Saturn', 'feschtlechi', 'Achti', 'ungr√§cht', 'baci', 'w√§√§rend', 'Konz√§rnuntern√§hm√§', 'Alt', 'China', 'Schwiizerische', 'Umgang', 'Riis', 'knudle', 'COMMANDER', 'Sch√∂neggstross', '7nehalb', 'regnet', 'Streetparade', 'pedagogische', '√§inzelne']\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "import json\n",
    "\n",
    "with open(\"../scripts/comments.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    comments = json.load(f)\n",
    "\n",
    "word_frequencies = {}\n",
    "for comment in comments:\n",
    "    sentence = comment[\"body\"]\n",
    "    for word in sentence.split(\" \"):\n",
    "        word = word.lower()\n",
    "        if word not in word_frequencies:\n",
    "            word_frequencies[word] = 1\n",
    "        else:\n",
    "            word_frequencies[word] += 1\n",
    "\n",
    "total_words = sum(word_frequencies.values())\n",
    "\n",
    "average_word_length = sum(len(word) * freq for word, freq in word_frequencies.items()) / total_words\n",
    "\n",
    "\n",
    "\n",
    "print(average_word_length, sep=\"\\n\")\n",
    "\n",
    "\n",
    "sorted(word_frequencies.items(), key=lambda x: x[1], reverse=True)[:100]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-05T00:46:55.842669Z",
     "start_time": "2023-07-05T00:46:55.432746Z"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
